{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2283406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Pycharm\\Thesis\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "OpenAI tiktoken module is not available for Python < 3.8,Linux ARM64 and AARCH64. Falling back to GPT2TokenizerFast.\n",
      "WARNING:haystack.nodes.answer_generator.openai:OpenAI tiktoken module is not available for Python < 3.8,Linux ARM64 and AARCH64. Falling back to GPT2TokenizerFast.\n",
      "Converting files: 100%|██████████████████████| 101/101 [00:15<00:00,  6.70it/s]\n",
      "Preprocessing:   6%|█▍                       | 6/101 [00:00<00:03, 27.32docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:We found one or more sentences whose word count is higher than the split length.\n",
      "Preprocessing:  28%|██████▋                 | 28/101 [00:00<00:00, 75.02docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:Document 454b06c4a4a14e7bf69467a6a4ac296c is 14317 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Preprocessing:  80%|███████████████████▏    | 81/101 [00:21<00:00, 41.30docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:Document af04dffb32043fa3fe8c1378149f6b33 is 11813 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 88a5b28b14b03ebec754f31d5a06b264 is 13906 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 7f27173ee3164ca2e6f34f9da0e3ab4a is 23471 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document f533370a6e66ba5989ad1b14683360f6 is 13477 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document ec7d749c185edfe1e6d93e6e794b9445 is 13580 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING:haystack.nodes.preprocessor.preprocessor:Document 77b311865d00e5bb8ab0dd0eb2e31768 is 10619 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Preprocessing: 100%|███████████████████████| 101/101 [00:50<00:00,  2.01docs/s]\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.nodes import FARMReader\n",
    "from pprint import pprint\n",
    "from haystack.utils import print_answers\n",
    "from haystack.utils import fetch_archive_from_http\n",
    "from haystack.nodes import BM25Retriever\n",
    "import os\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "from haystack.pipelines.standard_pipelines import TextIndexingPipeline\n",
    "\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "    similarity=\"dot_product\",\n",
    "    embedding_dim=768\n",
    ")\n",
    "\n",
    "doc_dir = \"corpus\\content\\manuals_dump\"\n",
    "\n",
    "files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "indexing_pipeline = TextIndexingPipeline(document_store)\n",
    "indexing_pipeline.run_batch(file_paths=files_to_index)\n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    model_format=\"sentence_transformers\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80f800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating embeddings:   0%|                       | 0/391320 [00:00<?, ? Docs/s]\n",
      "Batches:   0%|                                         | 0/313 [00:00<?, ?it/s]\u001b[A\n",
      "Batches:   0%|                               | 1/313 [00:38<3:19:43, 38.41s/it]\u001b[A\n",
      "Batches:   1%|▏                              | 2/313 [01:07<2:51:07, 33.01s/it]\u001b[A\n",
      "Batches:   1%|▎                              | 3/313 [01:37<2:42:42, 31.49s/it]\u001b[A\n",
      "Batches:   1%|▍                              | 4/313 [02:08<2:42:12, 31.50s/it]\u001b[A\n",
      "Batches:   2%|▍                              | 5/313 [02:37<2:37:17, 30.64s/it]\u001b[A\n",
      "Batches:   2%|▌                              | 6/313 [03:07<2:34:26, 30.18s/it]\u001b[A\n",
      "Batches:   2%|▋                              | 7/313 [03:36<2:32:35, 29.92s/it]\u001b[A\n",
      "Batches:   3%|▊                              | 8/313 [04:05<2:31:06, 29.73s/it]\u001b[A\n",
      "Batches:   3%|▉                              | 9/313 [04:35<2:30:28, 29.70s/it]\u001b[A\n",
      "Batches:   3%|▉                             | 10/313 [05:05<2:30:49, 29.87s/it]\u001b[A\n",
      "Batches:   4%|█                             | 11/313 [05:37<2:33:13, 30.44s/it]\u001b[A\n",
      "Batches:   4%|█▏                            | 12/313 [06:12<2:39:04, 31.71s/it]\u001b[A\n",
      "Batches:   4%|█▏                            | 13/313 [06:48<2:44:46, 32.96s/it]\u001b[A\n",
      "Batches:   4%|█▎                            | 14/313 [07:22<2:46:52, 33.49s/it]\u001b[A\n",
      "Batches:   5%|█▍                            | 15/313 [07:56<2:46:11, 33.46s/it]\u001b[A\n",
      "Batches:   5%|█▌                            | 16/313 [08:28<2:44:43, 33.28s/it]\u001b[A\n",
      "Batches:   5%|█▋                            | 17/313 [09:03<2:45:39, 33.58s/it]\u001b[A\n",
      "Batches:   6%|█▋                            | 18/313 [09:37<2:46:06, 33.78s/it]\u001b[A\n",
      "Batches:   6%|█▊                            | 19/313 [10:14<2:50:05, 34.71s/it]\u001b[A\n",
      "Batches:   6%|█▉                            | 20/313 [10:49<2:49:29, 34.71s/it]\u001b[A\n",
      "Batches:   7%|██                            | 21/313 [11:23<2:48:59, 34.72s/it]\u001b[A\n",
      "Batches:   7%|██                            | 22/313 [12:00<2:51:21, 35.33s/it]\u001b[A\n",
      "Batches:   7%|██▏                           | 23/313 [12:39<2:55:40, 36.35s/it]\u001b[A\n",
      "Batches:   8%|██▎                           | 24/313 [13:19<3:00:58, 37.57s/it]\u001b[A\n",
      "Batches:   8%|██▍                           | 25/313 [14:02<3:07:43, 39.11s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe.run(\n",
    "    query=\"How do I change the radio frequencies?\",\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 20},\n",
    "        \"Reader\": {\"top_k\": 20}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59949348",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "pprint(prediction)\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"minimum\" ## Choose from `minimum`, `medium`, and `all`\n",
    ")\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"all\" ## Choose from `minimum`, `medium`, and `all`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf036fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
